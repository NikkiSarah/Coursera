---
title: "Module 1 Assignment - Clustering"
output: html_document
date: "2023-05-01"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(factoextra)
library(cluster)
```

This is a rendered R Markdown document, which means that I coded my responses directly using R instead of in a GUI interface like Rattle. Please keep this in mind when marking my assignment as my results could therefore be different to the provided answer key. I have included my code (where relevant) to help in the peer review.

## Step 1: Download and process the data

```{r data, echo=FALSE}
data <- read_table("universities.txt",
                   col_names = c("university", "sat", "top10", "accept", "sfratio", "expenses", "grad"),
                   skip = 15,
                   show_col_types = FALSE)
data$sat <- data$sat * 100
data <- column_to_rownames(data, var = "university")
data
```

## Step 2: Perform K-Means clustering

```{r kmeans-1}
kmeans_data <- data %>% select(-grad)
scaled_kmeans_data <- scale(kmeans_data)

num_clusters = 10
wss = c()

for (k in 1:num_clusters) {
  mod <- kmeans(kmeans_data, centers = k)
  wss[k] <- mod$tot.withinss
}

wss_df <- tibble(k = 1:num_clusters, tot_wss = wss)
wss_df$diff_wss <- lag(wss_df$tot_wss) - wss_df$tot_wss
wss_df <- wss_df %>% gather("variable", "value", -k)
```

```{r scree-plot, warning=FALSE, echo=FALSE}
scree_plot <- ggplot(wss_df, aes(x = k, y = value, colour = variable, linetype = variable)) +
  geom_point(size = 3) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = 4, linetype = "dotted", size = 1) +
  geom_vline(xintercept = 3, linetype = "dotted", size = 1) +
  scale_x_continuous(breaks = 1:num_clusters) +
  scale_colour_manual(values = c("steelblue", "darkred"),
                      name = "Metric",
                      breaks = c("tot_wss", "diff_wss"),
                      labels = c("Total WSS", "Diff (Total WSS)")) +
  scale_linetype_manual(values = c("dashed", "solid")) +
  labs(x = "number of clusters", y = "value", title = "Scree plot") +
  guides(linetype = "none") +
  theme_classic() +
  theme(legend.position = "bottom") +
  annotate("text", label = "Optimal number of clusters", x = 5.5, y = 300000)
scree_plot
```

Choosing the optimal number of clusters can often involve a judggement call from the researcher. One of the most popular methods for determining the optimal number of clusters is locating the "elbow" or bend in the scree plot. In this case, it occurs at `k = 3`. An alternative method, as per the lectures, is where the total within sum of squares (Total WSS) and the difference in the Total WSS cross. Here, it's at `k = 4`.

## Step 3: Perform K-means clustering using the optimal number of clusters

```{r kmeans-2}
set.seed(42)

best_k <- 3
mod_3 <- kmeans(scaled_kmeans_data, centers = best_k)
print(mod_3$withinss)

best_k <- 4
mod_4 <- kmeans(scaled_kmeans_data, centers = best_k)
print(mod_4$withinss)
```

The numbers above display the within-cluster sum of squares values for a 3-cluster and 4-cluster model respectively. Notice how I used standardised data (variables that had been scaled to each have a mean of 0 and a standard deviation of 1). This is because the results of a k-means clustering algorithm will pay more attention to variables with larger magnitudes irrespective of their actual importance.

```{r visualise}
fviz_cluster(mod_4, scaled_kmeans_data, repel = TRUE, 
             ellipse.type = "confidence", ellipse.level = 0.999,
             palette = "Set1", main = "Discriminant plot",
             ggtheme = theme_classic())
```

After visualising the clusters (above), I determined that on the first two principal components (which explained 92.4% of the variability in the data), four clusters was possibly more appropriate. This was because Cal-Tech and John Hopkins sat slightly apart from the other three clusters.

## Step 4: Perform hierarchical clustering

In hierarchical clustering, there are several different linkage methods to choose from. However, since there is no one method that always outperforms the other, the agglomerative coefficient will be used to identify which one to use. In short, the agglomerative coefficient measures the strength of the clusters: the closer this value is to 1, the stronger the clusters.

The output below indicates that "Ward's" method resulted in the strongest coefficient for this dataset.

```{r linkage}
# define the linkage methods
m <- c("average", "single", "complete", "ward", "weighted", "gaverage")
names(m) <- c("average", "single", "complete", "ward", "weighted", "gaverage")

# calculate the agglomerative coefficient
calc_ac <- function(x) {
  agnes(scaled_kmeans_data, method = x)$ac
}

sapply(m, calc_ac)
```

The dendrogram below indicates that the dataset could reasonably be split up into 3 or 4 clusters, which is consistent with the results of the k-means clustering. To confirm this, we're going to calculate the gap statistic, which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.

```{r hierarchical-clustering, warning=FALSE}
clust <- agnes(scaled_kmeans_data, method ="ward")

fviz_dend(clust)
```

The plot indicates that the optimal number of clusters is 3, with 4 in second place. In order to be able to compare the resulting cluster assignments back to the k-means clustering results, we'll choose 4 clusters.

```{r gap-statistic}
gap_stat <- clusGap(scaled_kmeans_data, FUN = hcut, K.max = 7)
fviz_gap_stat(gap_stat)
```

```{r visualisation-2}
clust_4 <- hcut(scaled_kmeans_data, k = 4, hc_method = "ward.D")
fviz_cluster(clust_4, repel = TRUE,
             ellipse.type = "confidence", ellipse.level = 0.999,
             palette = "Set1", main = "Discriminant plot",
             ggtheme = theme_classic())
```

```{r comparison, echo=FALSE}
results <- kmeans_data %>% 
  mutate(hierarchical_clusters_2 = clust_4$cluster,
         kmeans_clusters = mod_4$cluster,
         hierarchical_clusters = clust_4$cluster)
# make it easier to compare cluster assignments
results$hierarchical_clusters[results$hierarchical_clusters == 4] <- 11
results$hierarchical_clusters[results$hierarchical_clusters == 3] <- 21
results$hierarchical_clusters[results$hierarchical_clusters == 1] <- 31
results$hierarchical_clusters[results$hierarchical_clusters == 2] <- 41

results$hierarchical_clusters[results$hierarchical_clusters == 11] <- 1
results$hierarchical_clusters[results$hierarchical_clusters == 21] <- 2
results$hierarchical_clusters[results$hierarchical_clusters == 31] <- 3
results$hierarchical_clusters[results$hierarchical_clusters == 41] <- 4
```

Whilst the plot above looks very similar to the one produced for the k-means clustering exercise, there are three very small differences in the cluster assignments. That is, Duke, Brown and Columbia universities all switched clusters.

The very final plot below shows the original data coloured by the hierarchical clustering assignments. It indicates that whilst some sort of separation can be observed between the red universities and all the others, there is little to indicate how the others differ on just dimensions. The one exception is when `sfratio` is one of the variables, which have slightly better separation between the four clusters.

```{r splom, message=FALSE}
GGally::ggpairs(results, mapping = aes(colour = factor(hierarchical_clusters)),
                columns = c(1:4),
                upper = "blank",
                diag = NULL) +
  theme_classic()
```