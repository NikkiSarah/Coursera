---
title: "Module 3 Assignment - Nearest Neighbours"
output: html_document
date: "2023-05-14"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r libraries}
library(tidyverse)
library(tidymodels)
```

## Step 1: Download and process the data

Here, we have read in the data. We can see that it is a mix of numeric and categorical variables, which will need to be processed prior to model fitting.

```{r data, echo=FALSE}
attrition <- read_csv("attrition.csv", show_col_types = FALSE)
glimpse(attrition)
```

The nearest neighbour algorithm requires solely numeric variables; however, to ensure that the representation of categorical variables is sensible, they need to be converted into ordered factors. This could be performed following data splitting during the construction of the recipe, but model documentation advises that this could be problematic and is better performed prior.

```{r factors}
attrition <- attrition %>% 
  mutate(BusinessTravel = factor(BusinessTravel,
                                 levels = c("Non-Travel", "Travel_Rarely", "Travel_Frequently"),
                                 ordered = TRUE),
         Education = factor(Education,
                            levels = c("Below_College", "College", "Bachelor", "Master", "Doctor"),
                            ordered = TRUE),
         WorkLifeBalance = factor(WorkLifeBalance,
                                  levels = c("Bad", "Better", "Good", "Best"),
                                  ordered = TRUE),
         PerformanceRating = factor(PerformanceRating,
                                    levels = c("Excellent", "Outstanding"),
                                    ordered = TRUE),
         # categoricals with the same level
         across(c(ends_with("Satisfaction"), JobInvolvement),
                ~factor(.x, levels = c("Low", "Medium", "High", "Very_High"),
                        ordered = TRUE)),
         # all other unordered categoricals
         across(where(is.character), as.factor)) %>% 
  # select required variables
  select(c(Attrition, Age, MonthlyIncome, DistanceFromHome, TotalWorkingYears,
           TrainingTimesLastYear, starts_with("Years")))
```

## Step 2: Split the data

In this step we have split the data into training, validation and testing sets using a (roughly) 80/10/10 split and stratified the outcome variable to ensure the distribution of outcome classes is not only similar across the training and testing sets, but representative of the original data.

The reason behind the triple-split is because model tuning should not be performed using a test dataset at the risk of obtaining overly optimistic performance results. Model tuning should be performed using a portion of the training data and the test data only used for final evaluation to simulate performance on an entirely unseen dataset.

```{r data_split}
set.seed(101)

data_split <- initial_split(attrition, prop = 0.9, strata = Attrition)
train_val_data <- training(data_split)
test_data <- testing(data_split)

val_split <- initial_split(train_val_data, prop = 0.9, strata = Attrition)
train_data <- training(val_split)
val_data <- testing(val_split)
```

# Step 3: Prepare and fit a base model

Nearest Neighbours is a distance-based algorithm which requires any categorical variables to be encoded into a numeric representation (such as via the creation of dummy variables) and normalisation of predictors to ensure that predictors with large values and/or large ranges of values don't unnecessarily influence model training.

```{r model-base}
knn_spec <- nearest_neighbor() %>% 
  set_mode("classification") %>% 
  set_engine("kknn")

# knn_rec <- recipe(Attrition ~., data = train_data) %>% 
#   # create dummy variables from factors with common levels
#   step_dummy_multi_choice(c(ends_with("Satisfaction"), JobInvolvement)) %>% 
#   # create dummy variables from all other factors
#   step_dummy(c(all_ordered_predictors(), all_unordered_predictors())) %>% 
#   # centre and scale all predictors
#   step_normalize(all_predictors())

knn_rec <- recipe(Attrition ~., data = train_val_data) %>%
  # centre and scale all predictors
  step_normalize(all_predictors())

knn_wflow <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(knn_rec)

fitted_knn <- knn_wflow %>% fit(data = train_val_data)
fitted_knn
```

```{r evaluate-base, cache=TRUE}
train_aug <- augment(fitted_knn, train_val_data)
metrics(train_aug, Attrition, .pred_class)

test_aug <- augment(fitted_knn, test_data)
metrics(test_aug, Attrition, .pred_class)
```

If a value of K is not specified for the KNN algorithm in `tidymodels`, then the default is 5. The results on the training and test data indicate that there is some overfitting occurring as the model achieved a training data accuracy of 91.1% but a test data accuracy of only 83.8%.

The next step will be to tune the value of K to determine if better performance is possible.

```{r tune-k}
# select 10 values of K; include the default
k_vals <- grid_regular(neighbors(range = c(1, 50)), levels = 25) %>% pull()
k_df <- data.frame()

knn_rec <- recipe(Attrition ~., data = train_data) %>%
  step_normalize(all_predictors())

for (val in k_vals) {
  knn_mod <- parsnip::nearest_neighbor(neighbors = val) %>% 
    set_mode("classification") %>% 
    set_engine("kknn")

  knn_wflow <- workflow() %>% 
    add_model(knn_mod) %>% 
    add_recipe(knn_rec) %>% 
    fit(data = train_data)
  
  fitted_knn <- knn_wflow %>% fit(data = train_data)
  
  train_preds <- augment(fitted_knn, train_data)
  train_acc <- metrics(train_preds, Attrition, .pred_class)[1,3]
  
  val_preds <- augment(fitted_knn, val_data)
  val_acc <- metrics(val_preds, Attrition, .pred_class)[1,3]
  
  test_preds <- augment(fitted_knn, test_data)
  test_acc <- metrics(test_preds, Attrition, .pred_class)[1,3]

  new_row <- data.frame(k = val,
                        train_acc = train_acc,
                        validation_acc = val_acc,
                        test_acc = test_acc)
  k_df <- rbind(k_df, new_row)
}  

names(k_df) <- c("k", "train", "validation", "test")

# plot the results
k_df %>% 
  select(-test) %>% 
  pivot_longer(!k, names_to = "data", values_to = "accuracy") %>% 
  arrange(k) %>% 
  ggplot(aes(x = k, y = accuracy, group = data, colour = data)) +
  geom_line(linewidth = 1) +
  geom_point(size = 4) +
  scale_colour_manual(name = "Dataset", values = c("darkcyan", "darkred")) +
  labs(x = "k", y = "accuracy score",
       title = "Average model accuracy across different numbers of neighbours") +
  theme_classic()
```

The results indicated that the best performance on the validation data was achived when there were 15 neighbours and the accuracy of the model was 82.7%. The accuracy of the training data was 86.7%. In both datasets, model performance quickly stabilised, but it was interesting to observe that as the number of neighbours increased, the difference in model performance reduced.

# Step 4: Evaluate model performance

The following chart includes the performance of the test set at different values of K. The fact that model performance on the test set is very similar to that of the validation set is further evidence that model overfitting was not occurring. Furthermore, accuracy of just over 80% indicated that the model was doing reasonably well at picking up important patterns in the data.

```{r evaluate}
# plot the results
k_df %>% 
  pivot_longer(!k, names_to = "data", values_to = "accuracy") %>% 
  arrange(k) %>% 
  ggplot(aes(x = k, y = accuracy, group = data, colour = data)) +
  geom_line(linewidth = 1) +
  geom_point(size = 4) +
  scale_colour_manual(name = "Dataset",
                      breaks = c("train", "validation", "test"),
                      values = c("darkcyan", "darkred", "steelblue")) +
  labs(x = "k", y = "accuracy score",
       title = "Average model accuracy across different numbers of neighbours") +
  theme_classic()
```