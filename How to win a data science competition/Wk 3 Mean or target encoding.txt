Mean encoding

1. Label encoding gives a random order. There is no correlation with the target value.
2. Mean encoding helps separate zeros from ones in a binary classification problem i.e. if a category appears five times and it's target value is one twice, then it should be encoded as
	2/5 or 0.4

Calculation approaches
- Likelhood = N(1) / (N(1) + N(0) = mean(target)
- Weight of evidence = ln(N(1) / N(0)) * 100
- Count = N(1) = sum(target)
- Diff = N(1) - N(0)


But, they cannot be used as is and require some sort of regularisation in the training set.
1. Cross-validation loop inside the training set
- intuitive and robust
- usually four or five folds are sufficient for decent results
- BUT there is some data leakage from the target variable, which becomes apparent if LOO scheme is used to separate the data

2. Smoothing
- based upon the following idea: if a category is large and has a lot of members, can trust estimated encoding, but if a category is small it's the opposite
- alpha controls the amount of regularisation 
	- when alpha = 0, there is no regularisation
	- when alpha is large, everything approaches the global mean
- alpha is equal to the category size we can trust
- (mean(target) * nrows + globalmean * alpha) / (nrows + alpha)
- but it's possible to use any formula that penalises encoding of categories can be considered smoothing
- smoothing can't be used on its own, for example combine it with CV loop regularisation

3. Add random noise
- adding noise degrades the quality of encoding on the training data
- BUT it's quite unstable and hard to make work
	- main issue is the amount of noise required. Too much turns the feature into garbage, but too little means insufficient regularisation
- usually used with LOO regularisation but needs diligent fine-tuning
- not the best option if time is limited

4. Sort and calculate an expanding mean.
- idea is to fix the sorting order of the data and use only rows from 0 to n-1 to calculate the encoding for row n
- least amount of leakage from target variable
- requires no hyperparameter tuning
- feature quality is not uniform, but this is not a huge issue as we can average models on encodings calculated from different data permutations
- also, CatBoost uses this method, which is an algorithm that performs wonderfully on datasets with categorical features

- in general, author prefers the CV loop or expanding mean method for practical tasks as they are the most robust and easy to tune


Regression Tasks
- mean encoding more flexible due to increased number of statistics that can be used (e.g. percentiles, std, distribution bins etc)

Multi-class Classification
- for every feature we wish to encode, there are n different encodings, where n is the number of clases

Time Series
- allows us to make many complicated features
- can encode using portions of training data e.g. mean of previous day, week etc.


Interactions and Numeric Features
- in practice, often been the case to encode numeric features in some combination
- i.e. bin and then treat as categorical
- in general, if using a dataset with lots of categorical variables, always worth trying mean encodings and mean-encoded interactions
	- interactions identified if occur on neighbouring nodes in a tree


